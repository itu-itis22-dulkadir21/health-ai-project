import numpy as np
import os
import cv2
import numpy as np
import matplotlib.animation as animation
import matplotlib.pyplot as plt
from numpy import sqrt, pi, arctan2, cos, sin
from scipy.ndimage import uniform_filter
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""TASK-A

1. HOG fonksiyonu (gÃ¶rÃ¼ntÃ¼den Ã¶zellik Ã§Ä±karma)
2. GÃ¶rÃ¼ntÃ¼ yÃ¼kleme fonksiyonu (load_ct_images)
3. K-means fonksiyonu (kÃ¼meleme)
4. GÃ¶rselleÅŸtirme fonksiyonlarÄ±

"""

def hog(image, orientations=9, pixels_per_cell=(8, 8),
        cells_per_block=(3, 3), normalise=False):
    """Extract Histogram of Oriented Gradients (HOG) for a given image.
    Compute a Histogram of Oriented Gradients (HOG) by
        1. (optional) global image normalisation
        2. computing the gradient image in x and y
        3. computing gradient histograms
        4. normalising across blocks
        5. flattening into a feature vector
    Parameters
    ----------
    image : (M, N) ndarray
        Input image (greyscale).
    orientations : int
        Number of orientation bins.
    pixels_per_cell : 2 tuple (int, int)
        Size (in pixels) of a cell.
    cells_per_block  : 2 tuple (int,int)
        Number of cells in each block.
    normalise : bool, optional
        Apply power law compression to normalise the image before
        processing.
    Returns
    -------
    newarr : ndarray
        HOG for the image as a 1D (flattened) array.
    """
    image = np.atleast_2d(image)
    """
    The first stage applies an optional global image normalisation
    equalisation that is designed to reduce the influence of illumination
    effects. In practice we use gamma (power law) compression, either
    computing the square root or the log of each colour channel.
    Image texture strength is typically proportional to the local surface
    illumination so this compression helps to reduce the effects of local
    shadowing and illumination variations.
    """

    if image.ndim > 3:
        raise ValueError("Currently only supports grey-level images")

    if normalise:
        image = sqrt(image)
    """
    The second stage computes first order image gradients. These capture
    contour, silhouette and some texture information, while providing
    further resistance to illumination variations. The locally dominant
    colour channel is used, which provides colour invariance to a large
    extent. Variant methods may also include second order image derivatives,
    which act as primitive bar detectors - a useful feature for capturing,
    e.g. bar like structures in bicycles and limbs in humans.
    """

    gx = np.zeros(image.shape)
    gy = np.zeros(image.shape)
    gx[:, :-1] = np.diff(image, n=1, axis=1)
    gy[:-1, :] = np.diff(image, n=1, axis=0)
    """
    The third stage aims to produce an encoding that is sensitive to
    local image content while remaining resistant to small changes in
    pose or appearance. The adopted method pools gradient orientation
    information locally in the same way as the SIFT [Lowe 2004]
    feature. The image window is divided into small spatial regions,
    called "cells". For each cell we accumulate a local 1-D histogram
    of gradient or edge orientations over all the pixels in the
    cell. This combined cell-level 1-D histogram forms the basic
    "orientation histogram" representation. Each orientation histogram
    divides the gradient angle range into a fixed number of
    predetermined bins. The gradient magnitudes of the pixels in the
    cell are used to vote into the orientation histogram.
    """

    magnitude = sqrt(gx ** 2 + gy ** 2)
    orientation = arctan2(gy, (gx + 1e-15)) * (180 / pi) + 90
    sx, sy = image.shape
    cx, cy = pixels_per_cell
    bx, by = cells_per_block

    n_cellsx = int(np.floor(sx // cx))  # number of cells in x
    n_cellsy = int(np.floor(sy // cy))  # number of cells in y

    # compute orientations integral images
    orientation_histogram = np.zeros((n_cellsx, n_cellsy, orientations))
    for i in range(orientations):
        temp_ori = np.where(orientation < 180 / orientations * (i + 1),
                            orientation, 0)
        temp_ori = np.where(orientation >= 180 / orientations * i,
                            temp_ori, 0)
        cond2 = temp_ori > 0
        temp_mag = np.where(cond2, magnitude, 0)
        orientation_histogram[:,:,i] = uniform_filter(temp_mag, size=(cx, cy))[int(cx/2)::cx, int(cy/2)::cy]
    n_blocksx = (n_cellsx - bx) + 1
    n_blocksy = (n_cellsy - by) + 1
    normalised_blocks = np.zeros((n_blocksx, n_blocksy,
                                  bx, by, orientations))

    for x in range(n_blocksx):
        for y in range(n_blocksy):
            block = orientation_histogram[x:x + bx, y:y + by, :]
            eps = 1e-5
            normalised_blocks[x, y, :] = block / sqrt(block.sum() ** 2 + eps)

    return normalised_blocks.ravel()


"""BURAYA KADAR OLAN KISIM HOG FONKSIYONU"""    
    

"""GÃ–RÃœNTÃœ YÃœKLEME FONKSIYONU"""
def load_ct_images(ct_directory, resize=True, target_size=(256, 256), enhance_contrast=True):
    """
    Load all CT images from a directory
    
    Parameters:
    -----------
    ct_directory : str
        Directory containing CT images
    resize : bool
        Whether to resize images to standard size
    target_size : tuple
        Size to resize images to if resize=True
    enhance_contrast : bool
        Whether to apply histogram equalization
        
    Returns:
    --------
    images : list
        List of loaded CT images as numpy arrays
    image_paths : list
        List of paths to the loaded images
    """
    # Check if directory exists
    if not os.path.exists(ct_directory):
        print(f"Error: Directory {ct_directory} does not exist")
        return [], []
    
    # Get all image files
    image_paths = []
    valid_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.dcm']
    
    for filename in os.listdir(ct_directory):
        file_ext = os.path.splitext(filename)[1].lower()
        if file_ext in valid_extensions:
            image_paths.append(os.path.join(ct_directory, filename))
    
    if not image_paths:
        print(f"No image files found in {ct_directory}")
        return [], []
    
    print(f"Found {len(image_paths)} CT images in {ct_directory}")
    
    # Load images
    images = []
    for img_path in image_paths:
        # Load image (grayscale)
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        
        if img is None:
            print(f"Error: Could not load image {img_path}")
            continue
        
        # Resize if requested
        if resize:
            img = cv2.resize(img, target_size)
        
        # Enhance contrast if requested
        if enhance_contrast:
            img = cv2.equalizeHist(img)
        
        images.append(img)
    
    print(f"Successfully loaded {len(images)} CT images")
    return images, image_paths

""" KMEANS FONKSIYONU"""

def kmeans(dataset, max_iterations=100, epsilon=0.001, distance='euclidean'):
    """
    K-means clustering algorithm optimized for brain CT image analysis using 3 clusters.
    Typically represents: normal tissue, potentially abnormal tissue, and background.
    
    Parameters:
    -----------
    dataset : ndarray
        Input dataset of shape (n_samples, n_features)
    max_iterations : int
        Maximum number of iterations to prevent infinite loops
    epsilon : float
        Convergence threshold
    distance : str
        Distance metric to use (currently only supports 'euclidean')
        
    Returns:
    --------
    centroids : ndarray
        Final cluster centroids
    history_centroids : list
        List of centroids at each iteration
    labels : ndarray
        Cluster assignments for each data point
    """
    # Fixed number of clusters for brain CT analysis
    k = 3
    
    # Initialize
    num_instances, num_features = dataset.shape
    centroids = dataset[np.random.choice(num_instances, k, replace=False)]
    history_centroids = [centroids.copy()]
    
    # Use 1D array for cluster assignments
    labels = np.zeros(num_instances, dtype=int)
    
    # Main loop
    for iteration in range(max_iterations):
        old_centroids = centroids.copy()
        
        # Assign points to nearest centroid
        for i, point in enumerate(dataset):
            # Calculate distance to each centroid
            distances = np.array([np.linalg.norm(point - centroid) for centroid in centroids])
            labels[i] = np.argmin(distances)
        
        # Update centroids
        new_centroids = np.zeros((k, num_features))
        for cluster in range(k):
            cluster_points = dataset[labels == cluster]
            if len(cluster_points) > 0:
                new_centroids[cluster] = cluster_points.mean(axis=0)
            else:
                # Handle empty clusters - reinitialize from farthest point
                distances = np.array([np.linalg.norm(dataset - centroids[j], axis=1) 
                                     for j in range(k)])
                distances = distances.sum(axis=0)
                new_centroids[cluster] = dataset[np.argmax(distances)]
        
        centroids = new_centroids
        history_centroids.append(centroids.copy())
        
        # Check convergence
        if np.linalg.norm(centroids - old_centroids) < epsilon:
            break
    
    return centroids, history_centroids, labels


"""VÄ°SUALÄ°ZÄ°NG"""
def visualize_anatomical_clusters(features, labels, centroids, feature_reduction=True):
    """
    Visualize brain CT clustering results by anatomical regions.
    
    Parameters:
    -----------
    features : ndarray
        HOG features or other high-dimensional data
    labels : ndarray
        Cluster assignments from K-means
    centroids : ndarray
        Final centroids from K-means
    feature_reduction : bool
        Whether to use PCA for dimensionality reduction
    """
    from sklearn.decomposition import PCA
    import matplotlib.pyplot as plt
    
    # Anatomical region colors and labels
    colors = ['#3498DB', '#AF7AC5', '#28B463']  # Blue, Purple, Green
    cluster_names = ['Eye Regions', 'Nasal/Sinus Areas', 'Brain Tissue']
    
    # Reduce dimensions for visualization if needed
    if feature_reduction and features.shape[1] > 2:
        pca = PCA(n_components=2)
        features_2d = pca.fit_transform(features)
        centroids_2d = pca.transform(centroids)
    else:
        features_2d = features
        centroids_2d = centroids
    
    # Plot clusters
    plt.figure(figsize=(10, 8))
    for i in range(3):  # Always 3 clusters
        cluster_points = features_2d[labels == i]
        plt.scatter(
            cluster_points[:, 0], 
            cluster_points[:, 1], 
            s=50, 
            c=colors[i],
            label=cluster_names[i]
        )
    
    # Plot centroids
    plt.scatter(
        centroids_2d[:, 0], 
        centroids_2d[:, 1], 
        s=200, 
        c='black', 
        marker='X', 
        label='Centroids'
    )
    
    plt.title('CT Image Anatomical Segmentation')
    plt.legend()
    plt.grid(alpha=0.3)
    plt.show()


def visualize_segmented_ct(image, labels, centroids):
    """
    Visualize anatomical segmentation directly on CT image.
    
    Parameters:
    -----------
    image : ndarray
        Original CT image
    labels : ndarray
        Cluster assignments from K-means
    centroids : ndarray
        Final cluster centroids
    """
    # Create a color-mapped segmentation mask
    h, w = image.shape
    segmentation = np.zeros((h, w, 3), dtype=np.uint8)
    
    # Color map for anatomical regions
    colors = [
        [52, 152, 219],   # Blue - Eye regions
        [175, 122, 197],  # Purple - Nasal/Sinus areas
        [40, 180, 99]     # Green - Brain tissue
    ]
    
    # Map HOG feature clusters back to image segments
    # This requires careful implementation based on how HOG features were extracted
    # A simplified approach would be to compute HOG for each pixel region
    # and assign it to the closest centroid
    
    # Display original and segmented images
    plt.figure(figsize=(14, 7))
    
    plt.subplot(1, 2, 1)
    plt.imshow(image, cmap='gray')
    plt.title('Original CT Image')
    plt.axis('off')
    
    plt.subplot(1, 2, 2)
    plt.imshow(segmentation)
    plt.title('Anatomical Segmentation')
    plt.axis('off')
    
    # Add a legend
    from matplotlib.patches import Patch
    legend_elements = [
        Patch(facecolor=np.array(colors[0])/255, label='Eye Regions'),
        Patch(facecolor=np.array(colors[1])/255, label='Nasal/Sinus Areas'),
        Patch(facecolor=np.array(colors[2])/255, label='Brain Tissue')
    ]
    plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))
    
    plt.tight_layout()
    plt.show()


"""TASK-B"""

import torch
import torchvision.models as models
from torchvision.models import ResNet50_Weights
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt

# ResNet50 modelini yÃ¼kle - son sÄ±nÄ±flandÄ±rma katmanÄ±nÄ± Ã§Ä±kar
weights = ResNet50_Weights.DEFAULT
model = models.resnet50(weights=weights)
# Son katmanÄ± kaldÄ±rÄ±p, Ã¶nceki katmanÄ±n Ã§Ä±ktÄ±sÄ±nÄ± almak iÃ§in
model = torch.nn.Sequential(*list(model.children())[:-1])  
model.eval()

# GÃ¶rÃ¼ntÃ¼ Ã¶n iÅŸleme fonksiyonu
def preprocess_image(img_path):
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    img = Image.open(img_path).convert('RGB')  # GÃ¶rÃ¼ntÃ¼yÃ¼ RGB formatÄ±nda aÃ§
    img_tensor = transform(img).unsqueeze(0)  # Batch boyutu ekle
    return img_tensor

# Ã–znitelik Ã§Ä±karma fonksiyonu
def extract_features(img_path):
    img_tensor = preprocess_image(img_path)
    with torch.no_grad():  # Gradyan hesaplama kapalÄ±
        features = model(img_tensor)
    return features.squeeze().numpy().flatten()  # 2048 boyutlu vektÃ¶r haline getir
  # TensÃ¶rÃ¼ numpy dizisine Ã§evir

# Kendi gÃ¶rÃ¼ntÃ¼ yÃ¼kleme fonksiyonunuzu kullanÄ±n
ct_directory = input("BT gÃ¶rÃ¼ntÃ¼lerinin bulunduÄŸu dizini girin: ")
_, image_paths = load_ct_images(ct_directory)

if not image_paths:
    print("GÃ¶rÃ¼ntÃ¼ bulunamadÄ± veya yÃ¼klenemedi.")
    exit()

# TÃ¼m gÃ¶rsellerden Ã¶znitelikleri Ã§Ä±kar
features = np.array([extract_features(path) for path in image_paths])

# DBSCAN ile kÃ¼meleme
dbscan = DBSCAN(eps=5, min_samples=2)
clusters = dbscan.fit_predict(features)

# SonuÃ§larÄ± gÃ¶rselleÅŸtir
plt.scatter(features[:, 0], features[:, 1], c=clusters, cmap='viridis')
plt.title("ResNet50 + DBSCAN KÃ¼meleme SonuÃ§larÄ±")
plt.show()

# KÃ¼meleri yazdÄ±r
for i, cluster in enumerate(clusters):
    print(f"GÃ¶rsel {image_paths[i]} -> KÃ¼me {cluster}")



"""TASK-C"""
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.metrics import structural_similarity as ssim
from scipy.cluster.hierarchy import fcluster, linkage
import seaborn as sns

# ðŸ“Œ 1ï¸âƒ£ GÃ¶rÃ¼ntÃ¼leri YÃ¼kleme ve Gri Tonlamaya Ã‡evirme
def load_gray_images(ct_directory, target_size=(256, 256)):
    """BT gÃ¶rÃ¼ntÃ¼lerini yÃ¼kleyip gri tonlamaya Ã§evirir."""
    image_paths = []
    images = []
    valid_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.dcm']

    for filename in os.listdir(ct_directory):
        if os.path.splitext(filename)[1].lower() in valid_extensions:
            img_path = os.path.join(ct_directory, filename)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Gri tonlama
            if img is not None:
                img = cv2.resize(img, target_size)
                images.append(img)
                image_paths.append(img_path)

    print(f"Toplam {len(images)} gÃ¶rÃ¼ntÃ¼ yÃ¼klendi.")
    return images, image_paths

# ðŸ“Œ 2ï¸âƒ£ SSIM Skor Matrisi Hesaplama
def compute_ssim_matrix(images):
    """GÃ¶rseller arasÄ±ndaki SSIM skor matrisini oluÅŸturur."""
    num_images = len(images)
    ssim_matrix = np.zeros((num_images, num_images))

    for i in range(num_images):
        for j in range(i, num_images):
            if i == j:
                ssim_matrix[i, j] = 1.0  # Kendisiyle benzerlik 1.0 olmalÄ±
            else:
                score, _ = ssim(images[i], images[j], full=True)
                ssim_matrix[i, j] = score
                ssim_matrix[j, i] = score  # Simetrik matris

    return ssim_matrix

# ðŸ“Œ 3ï¸âƒ£ Dynamic Threshold Selection
def find_optimal_threshold(ssim_matrix, alpha=0.5):
    """Dinamik olarak SSIM tabanlÄ± en iyi threshold'u belirler."""
    mean_ssim = np.mean(ssim_matrix)
    std_ssim = np.std(ssim_matrix)
    optimal_threshold = mean_ssim + alpha * std_ssim
    return optimal_threshold

# ðŸ“Œ 4ï¸âƒ£ SSIM Benzerliklerine GÃ¶re KÃ¼meleme
def cluster_images(ssim_matrix, threshold):
    """SSIM skorlarÄ±na gÃ¶re gÃ¶rÃ¼ntÃ¼leri gruplar."""
    linked = linkage(1 - ssim_matrix, method='ward')  # 1 - SSIM = mesafe
    clusters = fcluster(linked, threshold, criterion='distance')  # EÅŸik bazlÄ± kÃ¼meleme
    return clusters

# ðŸ“Œ 5ï¸âƒ£ SSIM IsÄ± HaritasÄ±nÄ± GÃ¶rselleÅŸtirme ve KÃ¼me GÃ¶sterimi
def visualize_clusters(images, image_paths, clusters, ssim_matrix):
    """KÃ¼meleme sonuÃ§larÄ±nÄ± gÃ¶rselleÅŸtirir."""
    # IsÄ± haritasÄ± gÃ¶ster
    plt.figure(figsize=(10, 8))
    sns.heatmap(ssim_matrix, annot=True, fmt=".2f", cmap="coolwarm",
                xticklabels=[os.path.basename(p) for p in image_paths],
                yticklabels=[os.path.basename(p) for p in image_paths])
    plt.title("SSIM Benzerlik Matrisi")
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()
    
    # KÃ¼meleri gÃ¶rselleÅŸtir
    cluster_dict = {}
    for i, (path, cluster) in enumerate(zip(image_paths, clusters)):
        cluster_dict.setdefault(cluster, []).append((i, os.path.basename(path)))
    
    # Her kÃ¼me iÃ§in Ã¶rnek gÃ¶rÃ¼ntÃ¼leri gÃ¶ster
    for cluster_id, items in cluster_dict.items():
        n_images = min(5, len(items))  # En fazla 5 gÃ¶rÃ¼ntÃ¼ gÃ¶ster
        if n_images > 0:
            plt.figure(figsize=(12, 3))
            plt.suptitle(f"KÃ¼me {cluster_id}")
            
            for i in range(n_images):
                idx, name = items[i]
                plt.subplot(1, n_images, i+1)
                plt.imshow(images[idx], cmap='gray')
                plt.title(name, fontsize=8)
                plt.axis('off')
            
            plt.tight_layout()
            plt.show()

# Ana iÅŸlev
ct_directory = input("BT gÃ¶rÃ¼ntÃ¼lerinin bulunduÄŸu dizini girin: ")
images, image_paths = load_gray_images(ct_directory)

if images:
    # SSIM matrisini hesapla
    ssim_matrix = compute_ssim_matrix(images)
    
    # Dinamik Threshold Belirleme
    optimal_threshold = find_optimal_threshold(ssim_matrix, alpha=0.5)
    print(f"Optimal Threshold: {optimal_threshold:.4f}")
    
    # Threshold ile KÃ¼meleme Yap
    clusters = cluster_images(ssim_matrix, threshold=optimal_threshold)
    
    # SonuÃ§larÄ± gÃ¶rselleÅŸtir
    visualize_clusters(images, image_paths, clusters, ssim_matrix)
else:
    print("GÃ¶rÃ¼ntÃ¼ bulunamadÄ± veya yÃ¼klenemedi.")

"""TASK-D: Autoencoder + t-SNE"""

import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.keras.layers import Input, Dense, Flatten, Reshape
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
import seaborn as sns

# ðŸ“Œ 1ï¸âƒ£ BT GÃ¶rÃ¼ntÃ¼lerini YÃ¼kleme ve HazÄ±rlama
def load_gray_images(ct_directory, target_size=(128, 128)):
    """BT gÃ¶rÃ¼ntÃ¼lerini yÃ¼kleyip gri tonlamaya Ã§evirir."""
    image_paths = []
    images = []
    valid_extensions = ['.jpg', '.jpeg', '.png', '.tif', '.dcm']

    for filename in os.listdir(ct_directory):
        if os.path.splitext(filename)[1].lower() in valid_extensions:
            img_path = os.path.join(ct_directory, filename)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is not None:
                img = cv2.resize(img, target_size)  # TÃ¼m gÃ¶rselleri aynÄ± boyuta getir
                images.append(img)
                image_paths.append(img_path)

    print(f"Toplam {len(images)} gÃ¶rÃ¼ntÃ¼ yÃ¼klendi.")
    images = np.array(images).astype("float32") / 255.0  # Normalizasyon
    images = np.expand_dims(images, axis=-1)  # Keras iÃ§in kanal boyutu ekle (128, 128, 1)
    return images, image_paths

# ðŸ“Œ 2ï¸âƒ£ Convolutional Autoencoder Modelini TanÄ±mla
def build_autoencoder(input_shape=(128, 128, 1), latent_dim=64):
    """Convolutional autoencoder mimarisini oluÅŸturur."""
    from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dense, Flatten, Reshape, Input
    
    # Encoder
    input_img = Input(shape=input_shape)
    
    # Convolutional layers
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)
    x = MaxPooling2D((2, 2), padding='same')(x)  # 64x64
    
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2), padding='same')(x)  # 32x32
    
    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)
    x = MaxPooling2D((2, 2), padding='same')(x)  # 16x16
    
    # Flatten and compress to latent space
    x = Flatten()(x)
    encoded = Dense(latent_dim, activation='relu')(x)
    
    # Decoder
    # First reconstruct the 3D shape
    x = Dense(16 * 16 * 8, activation='relu')(encoded)
    x = Reshape((16, 16, 8))(x)
    
    # Deconvolutional layers
    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)  # 32x32
    
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)  # 64x64
    
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)  # 128x128
    
    # Output layer
    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)
    
    # Models
    autoencoder = Model(input_img, decoded)
    encoder = Model(input_img, encoded)
    
    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss="mse")
    return autoencoder, encoder
# ðŸ“Œ 3ï¸âƒ£ Autoencoder EÄŸitme
def train_autoencoder(images, epochs=20, batch_size=16):
    """Autoencoder'Ä± verilen gÃ¶rÃ¼ntÃ¼lerle eÄŸitir."""
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
    
    # Early stopping ve model checkpoint tanÄ±mla
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
        ModelCheckpoint("autoencoder_best.h5", monitor="val_loss", save_best_only=True)
    ]
    
    # Model oluÅŸtur
    autoencoder, encoder = build_autoencoder(input_shape=images.shape[1:], latent_dim=64)
    
    # Model Ã¶zeti yazdÄ±r (opsiyonel)
    autoencoder.summary()
    
    # EÄŸitim - callbacks parametresi eklendi
    autoencoder.fit(
        images, images, 
        epochs=epochs, 
        batch_size=batch_size, 
        shuffle=True, 
        validation_split=0.2,
        callbacks=callbacks
    )
    
    return autoencoder, encoder

"""Underline code is OPTÄ°ONAL"""
# GÃ¶rÃ¼ntÃ¼ rekonstrÃ¼ksiyonlarÄ±nÄ± gÃ¶rselleÅŸtir
def plot_reconstructions(autoencoder, images, n=5):
    """Orijinal ve rekonstrÃ¼kte edilmiÅŸ gÃ¶rÃ¼ntÃ¼leri karÅŸÄ±laÅŸtÄ±r."""
    # Rastgele gÃ¶rÃ¼ntÃ¼ seÃ§
    indices = np.random.choice(range(len(images)), n, replace=False)
    sample_images = images[indices]
    
    # GÃ¶rÃ¼ntÃ¼leri rekonstrÃ¼kte et
    reconstructions = autoencoder.predict(sample_images)
    
    # GÃ¶rselleÅŸtir
    plt.figure(figsize=(15, 3))
    for i in range(n):
        # Orijinal
        plt.subplot(2, n, i+1)
        plt.imshow(sample_images[i].squeeze(), cmap='gray')
        plt.title("Orijinal")
        plt.axis('off')
        
        # RekonstrÃ¼ksiyon
        plt.subplot(2, n, i+n+1)
        plt.imshow(reconstructions[i].squeeze(), cmap='gray')
        plt.title("RekonstrÃ¼ksiyon")
        plt.axis('off')
    
    plt.tight_layout()
    plt.show()



# ðŸ“Œ 4ï¸âƒ£ Latent Uzaydan 64 Boyutlu Temsil Ã‡Ä±kartma
def extract_latent_features(encoder, images):
    """EÄŸitilen encoder modelinden 64 boyutlu latent Ã¶zellikleri Ã§Ä±karÄ±r."""
    return encoder.predict(images)

# ðŸ“Œ 5ï¸âƒ£ t-SNE ile 2D'ye Ä°ndirme ve KÃ¼meleme
def tsne_and_cluster(latent_features, image_paths):
    """Latent uzaydan 2D'ye indirgeme ve kÃ¼meleme yapar."""
    tsne = TSNE(n_components=2, perplexity=30, random_state=42)
    features_2d = tsne.fit_transform(latent_features)

    # K-means KÃ¼meleme (Opsiyonel)
    kmeans = KMeans(n_clusters=3, random_state=42)
    clusters = kmeans.fit_predict(latent_features)

    # GÃ¶rselleÅŸtirme
    plt.figure(figsize=(10, 8))
    sns.scatterplot(x=features_2d[:, 0], y=features_2d[:, 1], hue=clusters, palette="viridis", s=100)
    plt.title("Autoencoder + t-SNE KÃ¼meleme SonuÃ§larÄ±")
    plt.xlabel("t-SNE BileÅŸeni 1")
    plt.ylabel("t-SNE BileÅŸeni 2")
    
    # KÃ¼me isimlerini ekle
    for i, txt in enumerate([os.path.basename(p) for p in image_paths]):
        plt.annotate(txt, (features_2d[i, 0], features_2d[i, 1]), fontsize=8, alpha=0.7)

    plt.legend(title="KÃ¼me ID")
    plt.grid(alpha=0.3)
    plt.show()

# ðŸ“Œ **Ana Ä°ÅŸlem AdÄ±mlarÄ±**
ct_directory = input("BT gÃ¶rÃ¼ntÃ¼lerinin bulunduÄŸu dizini girin: ")
images, image_paths = load_gray_images(ct_directory)


if len(images) > 0:
    # **Autoencoder EÄŸit**
    autoencoder, encoder = train_autoencoder(images, epochs=20, batch_size=16)
    
    # RekonstrÃ¼ksiyonlarÄ± gÃ¶rselleÅŸtir
    plot_reconstructions(autoencoder, images, n=5)
    
    # **Latent Ã–zellikleri Ã‡Ä±kar**
    latent_features = extract_latent_features(encoder, images)
    print(f"Latent Uzay Åžekli: {latent_features.shape}")  # (num_images, 64)

    # **t-SNE ile 2Dâ€™ye Ä°ndir ve KÃ¼mele**
    tsne_and_cluster(latent_features, image_paths)
else:
    print("GÃ¶rÃ¼ntÃ¼ bulunamadÄ± veya yÃ¼klenemedi.")
